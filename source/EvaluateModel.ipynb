{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import AutomaticMetrics\n",
    "from Mimic import Preprocessor\n",
    "from Mimic import Mimic\n",
    "\n",
    "from Stupid import Stupid\n",
    "from Smart import Smart\n",
    "\n",
    "from Mimic_T import Mimic_T_Preprocessor\n",
    "from Mimic_T import Mimic_Trans\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "mic = Mimic.load('joeyData/joey400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 7117\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 3558\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 1779\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 889\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 444\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 222\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 111\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 55\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 27\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 13\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 6\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 3\n",
      "INFO:absl:SubwordTextEncoder build: trying min_token_count 1\n"
     ]
    }
   ],
   "source": [
    "mic_t = Mimic_Trans.load('../Ketan_Pramod/joey.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.990032906999293"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mic.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = 'joey'\n",
    "genericQuestions = pickle.load(\n",
    "    open('joeyData/genericQuestionsTest.pkl'.format(person), 'rb'))\n",
    "genericAnswers = pickle.load(\n",
    "    open('joeyData/genericAnswersTest.pkl'.format(person), 'rb'))\n",
    "personInput = pickle.load(\n",
    "    open('{}Data/{}InputTest.pkl'.format(person,person), 'rb'))\n",
    "personOutput = pickle.load(\n",
    "    open('{}Data/{}OutputTest.pkl'.format(person,person), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_mic_qa = AutomaticMetrics.AutomaticMetricTester(mic, genericQuestions, genericAnswers, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester_mic_qa.computeBLEUScoreAverage()\n",
    "# tester_mic_qa.computeBLEUScoreCorpus()\n",
    "# print(tester_mic_qa._bleu_score_average)\n",
    "# print(tester_mic_qa._bleu_score_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "tester_mic_qa.compileScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size:  112\n",
      "BLEU Corpus SCORE:  0.055007242371984295\n",
      "BLEU Avg. SCORE:  0.05297322878299355\n",
      "rouge 1 Recall Average:  0.22922904559168428\n",
      "rouge 1 Precision Average:  0.27251758428669626\n",
      "rouge 1 F1:  0.21339299762066238\n",
      "rouge 4 Recall Average:  0.044033843140986004\n",
      "rouge 4 Precision Average:  0.045238095238095244\n",
      "rouge 4 F1:  0.04444208877643007\n",
      "rouge el Recall Average:  0.21541890810286649\n",
      "rouge el Precision Average:  0.2565700847442354\n",
      "rouge el F1:  0.2039829569674685\n",
      "METEOR Precision:  0.14680063458487574\n",
      "METEOR Recall:  0.22573770491803283\n",
      "METEOR f1:  0.17790619014364248\n",
      "METEOR fmean:  0.2088892377766826\n",
      "METEOR Score:  0.09212926715053997\n",
      "WER Average Error:  1.5544307041549983\n"
     ]
    }
   ],
   "source": [
    "tester_mic_qa.printScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_mic_pi = AutomaticMetrics.AutomaticMetricTester(mic, personInput, personOutput, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "tester_mic_pi.compileScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size:  1537\n",
      "BLEU Corpus SCORE:  0.037124531650459015\n",
      "BLEU Avg. SCORE:  0.01534727090890753\n",
      "rouge 1 Recall Average:  0.16879866901994683\n",
      "rouge 1 Precision Average:  0.19572327808388063\n",
      "rouge 1 F1:  0.14131521311986808\n",
      "rouge 4 Recall Average:  0.01527719191509226\n",
      "rouge 4 Precision Average:  0.016478108796299313\n",
      "rouge 4 F1:  0.015664332128813196\n",
      "rouge el Recall Average:  0.15680875488110887\n",
      "rouge el Precision Average:  0.18259246794578293\n",
      "rouge el F1:  0.13103065494790853\n",
      "METEOR Precision:  0.12962537148311243\n",
      "METEOR Recall:  0.1530817674994838\n",
      "METEOR f1:  0.14038047324042713\n",
      "METEOR fmean:  0.14903642810508097\n",
      "METEOR Score:  0.06376018814092017\n",
      "WER Average Error:  1.9218250856994379\n"
     ]
    }
   ],
   "source": [
    "tester_mic_pi.printScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_trans_qa = AutomaticMetrics.AutomaticMetricTester(mic_t, genericQuestions, genericAnswers, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "tester_trans_qa.compileScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size:  112\n",
      "BLEU Corpus SCORE:  1.5083127795900138e-155\n",
      "BLEU Avg. SCORE:  5.546060695440674e-158\n",
      "rouge 1 Recall Average:  0.1563367750938484\n",
      "rouge 1 Precision Average:  0.12673974705268826\n",
      "rouge 1 F1:  0.10163660720082104\n",
      "rouge 4 Recall Average:  0.0\n",
      "rouge 4 Precision Average:  0.0\n",
      "rouge 4 F1:  0.0\n",
      "rouge el Recall Average:  0.14812313309911015\n",
      "rouge el Precision Average:  0.11469916354349555\n",
      "rouge el F1:  0.09369005177349107\n",
      "METEOR Precision:  0.09354838709677421\n",
      "METEOR Recall:  0.08456080254465377\n",
      "METEOR f1:  0.08882783314647323\n",
      "METEOR fmean:  0.08579723742551787\n",
      "METEOR Score:  0.03523108753709369\n",
      "WER Average Error:  2.4122750671486206\n"
     ]
    }
   ],
   "source": [
    "#print(tester_trans_qa._references)\n",
    "# print(tester_trans_qa._candidates)\n",
    "tester_trans_qa.printScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_trans_pi = AutomaticMetrics.AutomaticMetricTester(mic_t, personInput, personOutput, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "tester_trans_pi.compileScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size:  1537\n",
      "BLEU Corpus SCORE:  0.09956675661485048\n",
      "BLEU Avg. SCORE:  0.10961163969476855\n",
      "rouge 1 Recall Average:  0.2759844238581303\n",
      "rouge 1 Precision Average:  0.35776475233825694\n",
      "rouge 1 F1:  0.269806586380925\n",
      "rouge 4 Recall Average:  0.10791389292914157\n",
      "rouge 4 Precision Average:  0.10917895871922433\n",
      "rouge 4 F1:  0.10734408197261663\n",
      "rouge el Recall Average:  0.26699014502621055\n",
      "rouge el Precision Average:  0.3449883424118414\n",
      "rouge el F1:  0.26063202062991303\n",
      "METEOR Precision:  0.19424229266174553\n",
      "METEOR Recall:  0.31308381875893826\n",
      "METEOR f1:  0.2397436969318634\n",
      "METEOR fmean:  0.28676636352719276\n",
      "METEOR Score:  0.1372909639397772\n",
      "WER Average Error:  1.217962951323717\n"
     ]
    }
   ],
   "source": [
    "tester_trans_pi.printScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
