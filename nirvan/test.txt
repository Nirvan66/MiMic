# # encoder_input_data
# tokenized_questions = tokenizer.texts_to_sequences( questions )
# maxlen_questions = max( [ len(x) for x in tokenized_questions ] )
# padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )
# encoder_input_data = np.array( padded_questions )
# print( encoder_input_data.shape , maxlen_questions )

# # decoder_input_data
# tokenized_answers = tokenizer.texts_to_sequences( answers )
# maxlen_answers = max( [ len(x) for x in tokenized_answers ] )
# padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )
# decoder_input_data = np.array( padded_answers )
# print( decoder_input_data.shape , maxlen_answers )

# # decoder_output_data
# tokenized_answers = tokenizer.texts_to_sequences( answers )
# for i in range(len(tokenized_answers)) :
#     tokenized_answers[i] = tokenized_answers[i][1:]
# padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )
# onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )
# decoder_output_data = np.array( onehot_answers )
# print( decoder_output_data.shape )

mic = Mimic(Preprocessor())
preQ, preA = mic.build(questions[0:10],answers[0:10])
tokenizedInputs = mic.tokenizer.texts_to_sequences( preQ )
mx = max( [ len(x) for x in tokenizedInputs ] )
if mx>mic.maxInputLen:
    mic.maxInputLen = mx

tokenizedOutputs = mic.tokenizer.texts_to_sequences( preA )
mx = max( [ len(x) for x in tokenizedOutputs ] )
if mx>mic.maxOutputLen:
    mic.maxOutputLen = mx
paddedAnswers = preprocessing.sequence.pad_sequences( tokenizedOutputs , maxlen=mic.maxOutputLen , padding='post' )
decoderInput = np.array( paddedAnswers )

for w,i in mic.tokenizer.word_index.items():
    if len(e[w])>0:
        print((em[i]==e[w]))
    else:
        print(w)