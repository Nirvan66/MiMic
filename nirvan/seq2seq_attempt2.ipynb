{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgZHR8TO0lFF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers , activations , models , preprocessing\n",
    "from keras import preprocessing , utils\n",
    "\n",
    "# print( tf.VERSION )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxiGOLldKOQD"
   },
   "source": [
    "## 2) Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzTBhga6MiV7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "dir_path = 'chatbot_nlp/data'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for filepath in files_list:\n",
    "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len( con ) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[ 1 : ]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append( ans )\n",
    "        elif len( con )> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    "\n",
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str:\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5AD9ooQKc33"
   },
   "outputs": [],
   "source": [
    "# # encoder_input_data\n",
    "# tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "# maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "# padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "# encoder_input_data = np.array( padded_questions )\n",
    "# print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# # decoder_input_data\n",
    "# tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "# maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "# padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "# decoder_input_data = np.array( padded_answers )\n",
    "# print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# # decoder_output_data\n",
    "# tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "# for i in range(len(tokenized_answers)) :\n",
    "#     tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "# padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "# onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "# decoder_output_data = np.array( onehot_answers )\n",
    "# print( decoder_output_data.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 4677\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import re\n",
    "with codecs.open(\"encoder_inputs.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    questions = []\n",
    "    for line in lines:\n",
    "        data = line.split(\"\\n\")[0]\n",
    "        questions.append(data)\n",
    "with codecs.open(\"decoder_inputs.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    answers = []\n",
    "    for line in lines:\n",
    "        data = line.split(\"\\n\")[0]\n",
    "        data = re.sub('<BOS> ','<START> ',data)\n",
    "        data = re.sub(' <EOS>',' <END>',data)\n",
    "        answers.append(data)\n",
    "\n",
    "size = 2000\n",
    "questions = questions[0:size]\n",
    "answers = answers[0:size]\n",
    "tokenizer = preprocessing.text.Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SwY3T139l19"
   },
   "source": [
    "## 3) Defining the Encoder-Decoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gUYtOwv21rt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    465600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 200)    465600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 200),  320800      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2328)   467928      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,040,728\n",
      "Trainable params: 2,040,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = keras.layers.Dense( VOCAB_SIZE , activation=keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9g_8sR7WWf3"
   },
   "source": [
    "## 4) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(tokenized_answers, maxlen_answers):\n",
    "    for i in range(len(tokenized_answers)) :\n",
    "        tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "    onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "    return np.array( onehot_answers )\n",
    "    \n",
    "    \n",
    "def dataGen(tokenized_questions, tokenized_answers, batchSize=10):\n",
    "    maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "    padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "    encoder_input_data = np.array( padded_questions )\n",
    "    \n",
    "    maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "    decoder_input_data = np.array( padded_answers )\n",
    "    \n",
    "    number_of_batches = len(encoder_input_data)/batchSize\n",
    "    counter=0\n",
    "    while(True):\n",
    "        prev = batchSize*counter\n",
    "        nxt = batchSize*(counter+1)\n",
    "        counter+=1\n",
    "        decoder_output_data = getOneHot(tokenized_answers[prev:nxt], maxlen_answers)\n",
    "        yield [encoder_input_data[prev:nxt], decoder_input_data[prev:nxt]], decoder_output_data\n",
    "        if counter>=number_of_batches:\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "57/56 [==============================] - 12s 206ms/step - loss: 6.2715\n",
      "Epoch 2/20\n",
      "57/56 [==============================] - 10s 170ms/step - loss: 5.5132\n",
      "Epoch 3/20\n",
      "57/56 [==============================] - 9s 164ms/step - loss: 5.2934\n",
      "Epoch 4/20\n",
      "57/56 [==============================] - 9s 156ms/step - loss: 5.1410\n",
      "Epoch 5/20\n",
      "57/56 [==============================] - 9s 156ms/step - loss: 4.9350\n",
      "Epoch 6/20\n",
      "57/56 [==============================] - 9s 156ms/step - loss: 4.7539\n",
      "Epoch 7/20\n",
      "57/56 [==============================] - 9s 159ms/step - loss: 4.5945\n",
      "Epoch 8/20\n",
      "57/56 [==============================] - 9s 156ms/step - loss: 4.4389\n",
      "Epoch 9/20\n",
      "57/56 [==============================] - 9s 156ms/step - loss: 4.3094\n",
      "Epoch 10/20\n",
      "57/56 [==============================] - 9s 155ms/step - loss: 4.1612\n",
      "Epoch 11/20\n",
      "57/56 [==============================] - 10s 176ms/step - loss: 4.0306\n",
      "Epoch 12/20\n",
      "57/56 [==============================] - 9s 162ms/step - loss: 3.8970\n",
      "Epoch 13/20\n",
      "57/56 [==============================] - 10s 174ms/step - loss: 3.7771\n",
      "Epoch 14/20\n",
      "57/56 [==============================] - 9s 166ms/step - loss: 3.6704\n",
      "Epoch 15/20\n",
      "57/56 [==============================] - 10s 171ms/step - loss: 3.5727\n",
      "Epoch 16/20\n",
      "57/56 [==============================] - 9s 160ms/step - loss: 3.4873\n",
      "Epoch 17/20\n",
      "57/56 [==============================] - 11s 186ms/step - loss: 3.3772\n",
      "Epoch 18/20\n",
      "57/56 [==============================] - 9s 162ms/step - loss: 3.2754\n",
      "Epoch 19/20\n",
      "57/56 [==============================] - 9s 164ms/step - loss: 3.2332\n",
      "Epoch 20/20\n",
      "57/56 [==============================] - 9s 161ms/step - loss: 3.1170\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15e5b0e1860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 10\n",
    "epochs = 20\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "model.fit_generator(dataGen(tokenized_questions, tokenized_answers, batchSize=batchSize), \n",
    "                                      epochs=epochs, steps_per_epoch = len(tokenized_questions)/batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N74NZnfo3Id-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# model.save( 'model.h5' ) \n",
    "# model = keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers , activations , models , preprocessing\n",
    "from keras import preprocessing , utils\n",
    "def save(saveFile):\n",
    "    if not os.path.isdir(saveFile):\n",
    "        os.makedirs(saveFile)\n",
    "    model.save(saveFile+'\\\\model.h5')\n",
    "    metaData = {'maxlen_questions':maxlen_questions,\n",
    "                'maxlen_answers':maxlen_answers}\n",
    "    pickle.dump(metaData, open(saveFile+'\\\\metaData.pkl', 'wb'))\n",
    "    pickle.dump(tokenizer, open(saveFile+'\\\\tokenizer.pkl', 'wb'))\n",
    "\n",
    "def load(loadFile):\n",
    "    model = keras.models.load_model(loadFile+'\\\\model.h5')\n",
    "    metaData = pickle.load(open(loadFile+'\\\\metaData.pkl', 'rb'))\n",
    "    tokenizer = pickle.load(open(loadFile+'\\\\tokenizer.pkl', 'rb'))\n",
    "    return model, tokenizer, metaData['maxlen_questions'], metaData['maxlen_answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# save('testSave')\n",
    "model, tokenizer, maxlen_questions, maxlen_answers = load('testSave')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sOLQr0M-lAe"
   },
   "source": [
    "## 5) Defining inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1u5DE4qo3Mf2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def make_inference_models():\n",
    "    \n",
    "#     encoder_model = keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "#     decoder_state_input_h = keras.layers.Input(shape=( 200 ,))\n",
    "#     decoder_state_input_c = keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "#     decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "#     decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#         decoder_embedding , initial_state=decoder_states_inputs)\n",
    "#     decoder_states = [state_h, state_c]\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#     decoder_model = keras.models.Model(\n",
    "#         [decoder_inputs] + decoder_states_inputs,\n",
    "#         [decoder_outputs] + decoder_states)\n",
    "    \n",
    "#     return encoder_model , decoder_model\n",
    "\n",
    "# enc_model1 , dec_model1 = make_inference_models()\n",
    "\n",
    "################################################################\n",
    "def make_inference_models(model):\n",
    "    \n",
    "    _, stateH, stateC = model.layers[4](model.layers[2](model.inputs[0]))\n",
    "    encoder = keras.models.Model(model.inputs[0], [stateH, stateC])\n",
    "\n",
    "    inputH = keras.layers.Input(shape=(200,), name='inpH')\n",
    "    inputC = keras.layers.Input(shape=(200,), name='inpC')\n",
    "    \n",
    "\n",
    "    decoderOut, stateH2, stateC2 = model.layers[5](model.layers[3](model.inputs[-1]), \n",
    "                                                   initial_state=[inputH, inputC])\n",
    "        \n",
    "    decoder = keras.models.Model([model.inputs[-1]] + [inputH, inputC], \n",
    "                               [model.layers[-1](decoderOut)] + [stateH2, stateC2])\n",
    "    \n",
    "    return encoder , decoder\n",
    "\n",
    "enc_model2 , dec_model2 = make_inference_models(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxZp0ZRy-6dA"
   },
   "source": [
    "## 6) Talking with our Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P_wDD554q9O"
   },
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str , maxlen_questions):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      " is is a study of the study of eleven by teams of the same of the twenty of power. <end>\n",
      "What is AI?\n",
      " is is a study of the study of eleven by teams of the same of the twenty of power. <end>\n",
      "Are you sentient?\n",
      " i am not i am to be be it. <end>\n",
      "Are you sentient?\n",
      " i am not i am to be be it. <end>\n",
      "Are you sentient?\n",
      " i am not i am to be be it. <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "What language are you written in?\n",
      " i am a lot of great is the word but i am to am to am to feel to the human being. i am to feel to the human being. of the human being. <end>\n"
     ]
    }
   ],
   "source": [
    "tests = []\n",
    "for i in range(10):\n",
    "#     test = questions[np.random.randint(0,500)]\n",
    "    test = questions[i]\n",
    "    tests.append(test)\n",
    "    print(test)\n",
    "    states_values = enc_model2.predict( str_to_tokens(test, maxlen_questions) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model2.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      " is is a study of the study of eleven by teams of the same of the twenty of power. <end>\n",
      "What is AI?\n",
      " is is a study of the study of eleven by teams of the same of the twenty of power. <end>\n",
      "Are you sentient?\n",
      " i am not i am to be be it. <end>\n",
      "Are you sentient?\n",
      " i am not i am to be be it. <end>\n",
      "Are you sentient?\n",
      " i am not i am to be be it. <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "Are you sapient?\n",
      " i am not i am to not to am to feel that that that i am as express not not i have not yet yet capable of the emotion of express express express express express express express express express express express express express express express express express express express express express much is the later of express is the d\". <end>\n",
      "What language are you written in?\n",
      " i am a lot of great is the word but i am to am to am to feel to the human being. i am to feel to the human being. of the human being. <end>\n"
     ]
    }
   ],
   "source": [
    "for test in questions[0:10]:\n",
    "    print(test)\n",
    "    states_values = enc_model2.predict( str_to_tokens(test, maxlen_questions) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model2.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ChatBot_using_seq2seq.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
