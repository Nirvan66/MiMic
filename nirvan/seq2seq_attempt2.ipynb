{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgZHR8TO0lFF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers , activations , models , preprocessing\n",
    "from keras import preprocessing , utils\n",
    "\n",
    "# print( tf.VERSION )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxiGOLldKOQD"
   },
   "source": [
    "## 2) Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzTBhga6MiV7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 2328\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "dir_path = 'C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\chatbot_nlp\\\\data'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for filepath in files_list:\n",
    "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len( con ) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[ 1 : ]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append( ans )\n",
    "        elif len( con )> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    "\n",
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str:\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5AD9ooQKc33"
   },
   "outputs": [],
   "source": [
    "# # encoder_input_data\n",
    "# tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "# maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "# padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "# encoder_input_data = np.array( padded_questions )\n",
    "# print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# # decoder_input_data\n",
    "# tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "# maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "# padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "# decoder_input_data = np.array( padded_answers )\n",
    "# print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# # decoder_output_data\n",
    "# tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "# for i in range(len(tokenized_answers)) :\n",
    "#     tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "# padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "# onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "# decoder_output_data = np.array( onehot_answers )\n",
    "# print( decoder_output_data.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 4677\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import re\n",
    "with codecs.open(\"encoder_inputs.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    questions = []\n",
    "    for line in lines:\n",
    "        data = line.split(\"\\n\")[0]\n",
    "        questions.append(data)\n",
    "with codecs.open(\"decoder_inputs.txt\", \"rb\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    answers = []\n",
    "    for line in lines:\n",
    "        data = line.split(\"\\n\")[0]\n",
    "        data = re.sub('<BOS> ','<START> ',data)\n",
    "        data = re.sub(' <EOS>',' <END>',data)\n",
    "        answers.append(data)\n",
    "\n",
    "size = 2000\n",
    "questions = questions[0:size]\n",
    "answers = answers[0:size]\n",
    "tokenizer = preprocessing.text.Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SwY3T139l19"
   },
   "source": [
    "## 3) Defining the Encoder-Decoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gUYtOwv21rt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    465600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 200)    465600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 200),  320800      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2328)   467928      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,040,728\n",
      "Trainable params: 2,040,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = keras.layers.Dense( VOCAB_SIZE , activation=keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9g_8sR7WWf3"
   },
   "source": [
    "## 4) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(tokenized_answers, maxlen_answers):\n",
    "    for i in range(len(tokenized_answers)) :\n",
    "        tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "    onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "    return np.array( onehot_answers )\n",
    "    \n",
    "    \n",
    "def dataGen(tokenized_questions, tokenized_answers, batchSize=10):\n",
    "    maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "    padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "    encoder_input_data = np.array( padded_questions )\n",
    "    \n",
    "    maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "    decoder_input_data = np.array( padded_answers )\n",
    "    \n",
    "    number_of_batches = len(encoder_input_data)/batchSize\n",
    "    counter=0\n",
    "    while(True):\n",
    "        prev = batchSize*counter\n",
    "        nxt = batchSize*(counter+1)\n",
    "        counter+=1\n",
    "        decoder_output_data = getOneHot(tokenized_answers[prev:nxt], maxlen_answers)\n",
    "        yield [encoder_input_data[prev:nxt], decoder_input_data[prev:nxt]], decoder_output_data\n",
    "        if counter>=number_of_batches:\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      "57/56 [==============================] - 14s 244ms/step - loss: 6.2577\n",
      "Epoch 2/20\n",
      "57/56 [==============================] - 11s 198ms/step - loss: 5.5186\n",
      "Epoch 3/20\n",
      "57/56 [==============================] - 11s 198ms/step - loss: 5.2886\n",
      "Epoch 4/20\n",
      "57/56 [==============================] - 11s 199ms/step - loss: 5.1580\n",
      "Epoch 5/20\n",
      "57/56 [==============================] - 12s 204ms/step - loss: 4.9476\n",
      "Epoch 6/20\n",
      "57/56 [==============================] - 12s 207ms/step - loss: 4.7675\n",
      "Epoch 7/20\n",
      "57/56 [==============================] - 12s 206ms/step - loss: 4.6124\n",
      "Epoch 8/20\n",
      "57/56 [==============================] - 12s 209ms/step - loss: 4.4584\n",
      "Epoch 9/20\n",
      "57/56 [==============================] - 12s 218ms/step - loss: 4.3271\n",
      "Epoch 10/20\n",
      "57/56 [==============================] - 13s 229ms/step - loss: 4.1852\n",
      "Epoch 11/20\n",
      "57/56 [==============================] - 13s 220ms/step - loss: 4.0524\n",
      "Epoch 12/20\n",
      "57/56 [==============================] - 11s 198ms/step - loss: 3.9408\n",
      "Epoch 13/20\n",
      "57/56 [==============================] - 11s 196ms/step - loss: 3.8270\n",
      "Epoch 14/20\n",
      "57/56 [==============================] - 11s 198ms/step - loss: 3.7376\n",
      "Epoch 15/20\n",
      "57/56 [==============================] - 11s 196ms/step - loss: 3.6201\n",
      "Epoch 16/20\n",
      "57/56 [==============================] - 11s 201ms/step - loss: 3.5102\n",
      "Epoch 17/20\n",
      "57/56 [==============================] - 11s 198ms/step - loss: 3.4187\n",
      "Epoch 18/20\n",
      "57/56 [==============================] - 11s 201ms/step - loss: 3.3493\n",
      "Epoch 19/20\n",
      "57/56 [==============================] - 11s 199ms/step - loss: 3.2500\n",
      "Epoch 20/20\n",
      "57/56 [==============================] - 11s 200ms/step - loss: 3.1383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16aefb229b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 10\n",
    "epochs = 20\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "model.fit_generator(dataGen(tokenized_questions, tokenized_answers, batchSize=batchSize), \n",
    "                                      epochs=epochs, steps_per_epoch = len(tokenized_questions)/batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers , activations , models , preprocessing\n",
    "from keras import preprocessing , utils\n",
    "def save(saveFile):\n",
    "    if not os.path.isdir(saveFile):\n",
    "        os.makedirs(saveFile)\n",
    "    model.save(saveFile+'\\\\model.h5')\n",
    "    metaData = {'maxlen_questions':maxlen_questions,\n",
    "                'maxlen_answers':maxlen_answers}\n",
    "    pickle.dump(metaData, open(saveFile+'\\\\metaData.pkl', 'wb'))\n",
    "    pickle.dump(tokenizer, open(saveFile+'\\\\tokenizer.pkl', 'wb'))\n",
    "\n",
    "def load(loadFile):\n",
    "    model = keras.models.load_model(loadFile+'\\\\model.h5')\n",
    "    metaData = pickle.load(open(loadFile+'\\\\metaData.pkl', 'rb'))\n",
    "    tokenizer = pickle.load(open(loadFile+'\\\\tokenizer.pkl', 'rb'))\n",
    "    return model, tokenizer, metaData['maxlen_questions'], metaData['maxlen_answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# save('testSave')\n",
    "# model, tokenizer, maxlen_questions, maxlen_answers = load('testSave')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sOLQr0M-lAe"
   },
   "source": [
    "## 5) Defining inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1u5DE4qo3Mf2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def make_inference_models():\n",
    "    \n",
    "#     encoder_model = keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "#     decoder_state_input_h = keras.layers.Input(shape=( 200 ,))\n",
    "#     decoder_state_input_c = keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "#     decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "#     decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#         decoder_embedding , initial_state=decoder_states_inputs)\n",
    "#     decoder_states = [state_h, state_c]\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#     decoder_model = keras.models.Model(\n",
    "#         [decoder_inputs] + decoder_states_inputs,\n",
    "#         [decoder_outputs] + decoder_states)\n",
    "    \n",
    "#     return encoder_model , decoder_model\n",
    "\n",
    "# enc_model1 , dec_model1 = make_inference_models()\n",
    "\n",
    "################################################################\n",
    "def make_inference_models(model):\n",
    "    \n",
    "    _, stateH, stateC = model.layers[4](model.layers[2](model.inputs[0]))\n",
    "    encoder = keras.models.Model(model.inputs[0], [stateH, stateC])\n",
    "\n",
    "    inputH = keras.layers.Input(shape=(200,), name='inpH')\n",
    "    inputC = keras.layers.Input(shape=(200,), name='inpC')\n",
    "    \n",
    "\n",
    "    decoderOut, stateH2, stateC2 = model.layers[5](model.layers[3](model.inputs[-1]), \n",
    "                                                   initial_state=[inputH, inputC])\n",
    "        \n",
    "    decoder = keras.models.Model([model.inputs[-1]] + [inputH, inputC], \n",
    "                               [model.layers[-1](decoderOut)] + [stateH2, stateC2])\n",
    "    \n",
    "    return encoder , decoder\n",
    "\n",
    "enc_model2 , dec_model2 = make_inference_models(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxZp0ZRy-6dA"
   },
   "source": [
    "## 6) Talking with our Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P_wDD554q9O"
   },
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str , maxlen_questions):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      " hal is the science of science of science of two teams of the twenty of production of the twenty of production of the products. of the same of production of the sun, of the products. of the same of production of the sun, of the products. of the same of production of the twenty of production of the products. of the same of production of the sun, of the products. of the same of production\n",
      "What is AI?\n",
      " hal is the science of science of science of two teams of the twenty of production of the twenty of production of the products. of the same of production of the sun, of the products. of the same of production of the sun, of the products. of the same of production of the twenty of production of the products. of the same of production of the sun, of the products. of the same of production\n",
      "Are you sentient?\n",
      " i am a real kingdom of great to the word of the d\". <end>\n",
      "Are you sentient?\n",
      " i am a real kingdom of great to the word of the d\". <end>\n",
      "Are you sentient?\n",
      " i am a real kingdom of great to the word of the d\". <end>\n",
      "Are you sapient?\n",
      " i am a real kingdom of the computer, of topics, <end>\n",
      "Are you sapient?\n",
      " i am a real kingdom of the computer, of topics, <end>\n",
      "Are you sapient?\n",
      " i am a real kingdom of the computer, of topics, <end>\n",
      "Are you sapient?\n",
      " i am a real kingdom of the computer, of topics, <end>\n",
      "What language are you written in?\n",
      " what is a bad spouse <end>\n"
     ]
    }
   ],
   "source": [
    "tests = []\n",
    "for i in range(10):\n",
    "#     test = questions[np.random.randint(0,500)]\n",
    "    test = questions[i]\n",
    "    tests.append(test)\n",
    "    print(test)\n",
    "    states_values = enc_model1.predict( str_to_tokens(test, maxlen_questions) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model1.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ChatBot_using_seq2seq.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
