{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UgZHR8TO0lFF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers , activations , models , preprocessing\n",
    "from keras import preprocessing , utils\n",
    "# print( tf.VERSION )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxiGOLldKOQD"
   },
   "source": [
    "## 2) Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzTBhga6MiV7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 2326\n"
     ]
    }
   ],
   "source": [
    "questions = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\genericQuestions.pkl', 'rb'))\n",
    "answers = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\genericAnswers.pkl', 'rb'))\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 4677\n"
     ]
    }
   ],
   "source": [
    "# questions = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\movieInput.pkl', 'rb'))\n",
    "# answers = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\movieOutput.pkl', 'rb'))\n",
    "\n",
    "# size = 2000\n",
    "# questions = questions[0:size]\n",
    "# answers = answers[0:size]\n",
    "# tokenizer = preprocessing.text.Tokenizer(filters='\\t\\n')\n",
    "# tokenizer.fit_on_texts( questions + answers )\n",
    "# VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "# print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SwY3T139l19"
   },
   "source": [
    "## 3) Defining the Encoder-Decoder model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gUYtOwv21rt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    465600      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 200)    465600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 200),  320800      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2328)   467928      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,040,728\n",
      "Trainable params: 2,040,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_inputs = keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = keras.layers.Dense( VOCAB_SIZE , activation=keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9g_8sR7WWf3"
   },
   "source": [
    "## 4) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneHot(tokenized_answers, maxlen_answers):\n",
    "    for i in range(len(tokenized_answers)) :\n",
    "        tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "    onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "    return np.array( onehot_answers )\n",
    "    \n",
    "    \n",
    "def dataGen(tokenized_questions, tokenized_answers, batchSize=10):\n",
    "    maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "    padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='pre' )\n",
    "    encoder_input_data = np.array( padded_questions )\n",
    "    \n",
    "    maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "    decoder_input_data = np.array( padded_answers )\n",
    "    \n",
    "    number_of_batches = len(encoder_input_data)/batchSize\n",
    "    counter=0\n",
    "    while(True):\n",
    "        prev = batchSize*counter\n",
    "        nxt = batchSize*(counter+1)\n",
    "        counter+=1\n",
    "        decoder_output_data = getOneHot(tokenized_answers[prev:nxt], maxlen_answers)\n",
    "        yield [encoder_input_data[prev:nxt], decoder_input_data[prev:nxt]], decoder_output_data\n",
    "        if counter>=number_of_batches:\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/10\n",
      "57/56 [==============================] - 10s 172ms/step - loss: 1.2147\n",
      "Epoch 2/10\n",
      "57/56 [==============================] - 9s 160ms/step - loss: 1.0915\n",
      "Epoch 3/10\n",
      "57/56 [==============================] - 9s 153ms/step - loss: 1.0550\n",
      "Epoch 4/10\n",
      "57/56 [==============================] - 9s 153ms/step - loss: 1.0261\n",
      "Epoch 5/10\n",
      "57/56 [==============================] - 9s 161ms/step - loss: 0.9966\n",
      "Epoch 6/10\n",
      "57/56 [==============================] - 9s 153ms/step - loss: 0.9664\n",
      "Epoch 7/10\n",
      "57/56 [==============================] - 9s 150ms/step - loss: 0.9422\n",
      "Epoch 8/10\n",
      "57/56 [==============================] - 9s 152ms/step - loss: 0.9179\n",
      "Epoch 9/10\n",
      "57/56 [==============================] - 9s 151ms/step - loss: 0.8913\n",
      "Epoch 10/10\n",
      "57/56 [==============================] - 9s 160ms/step - loss: 0.8660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b38cea1128>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchSize = 10\n",
    "epochs = 10\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "model.fit_generator(dataGen(tokenized_questions, tokenized_answers, batchSize=batchSize), \n",
    "                                      epochs=epochs, steps_per_epoch = len(tokenized_questions)/batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers , activations , models , preprocessing\n",
    "from keras import preprocessing , utils\n",
    "def save(saveFile):\n",
    "    if not os.path.isdir(saveFile):\n",
    "        os.makedirs(saveFile)\n",
    "    model.save(saveFile+'\\\\model.h5')\n",
    "    metaData = {'maxlen_questions':maxlen_questions,\n",
    "                'maxlen_answers':maxlen_answers}\n",
    "    pickle.dump(metaData, open(saveFile+'\\\\metaData.pkl', 'wb'))\n",
    "    pickle.dump(tokenizer, open(saveFile+'\\\\tokenizer.pkl', 'wb'))\n",
    "\n",
    "def load(loadFile):\n",
    "    model = keras.models.load_model(loadFile+'\\\\model.h5')\n",
    "    metaData = pickle.load(open(loadFile+'\\\\metaData.pkl', 'rb'))\n",
    "    tokenizer = pickle.load(open(loadFile+'\\\\tokenizer.pkl', 'rb'))\n",
    "    return model, tokenizer, metaData['maxlen_questions'], metaData['maxlen_answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# save('testSave')\n",
    "# model, tokenizer, maxlen_questions, maxlen_answers = load('testSave')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3sOLQr0M-lAe"
   },
   "source": [
    "## 5) Defining inference models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1u5DE4qo3Mf2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def make_inference_models():\n",
    "    \n",
    "#     encoder_model = keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "#     decoder_state_input_h = keras.layers.Input(shape=( 200 ,))\n",
    "#     decoder_state_input_c = keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "#     decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "#     decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#         decoder_embedding , initial_state=decoder_states_inputs)\n",
    "#     decoder_states = [state_h, state_c]\n",
    "#     decoder_outputs = decoder_dense(decoder_outputs)\n",
    "#     decoder_model = keras.models.Model(\n",
    "#         [decoder_inputs] + decoder_states_inputs,\n",
    "#         [decoder_outputs] + decoder_states)\n",
    "    \n",
    "#     return encoder_model , decoder_model\n",
    "\n",
    "# enc_model1 , dec_model1 = make_inference_models()\n",
    "\n",
    "################################################################\n",
    "def make_inference_models(model):\n",
    "    \n",
    "    _, stateH, stateC = model.layers[4](model.layers[2](model.inputs[0]))\n",
    "    encoder = keras.models.Model(model.inputs[0], [stateH, stateC])\n",
    "\n",
    "    inputH = keras.layers.Input(shape=(200,), name='inpH')\n",
    "    inputC = keras.layers.Input(shape=(200,), name='inpC')\n",
    "    \n",
    "\n",
    "    decoderOut, stateH2, stateC2 = model.layers[5](model.layers[3](model.inputs[-1]), \n",
    "                                                   initial_state=[inputH, inputC])\n",
    "        \n",
    "    decoder = keras.models.Model([model.inputs[-1]] + [inputH, inputC], \n",
    "                               [model.layers[-1](decoderOut)] + [stateH2, stateC2])\n",
    "    \n",
    "    return encoder , decoder\n",
    "\n",
    "enc_model2 , dec_model2 = make_inference_models(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxZp0ZRy-6dA"
   },
   "source": [
    "## 6) Talking with our Chatbot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P_wDD554q9O"
   },
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str , maxlen_questions):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      " is the the science of science of production of the high of society. of production of the high of various phase. of various society. attack hypothetical bioinformatics sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun\n",
      "What is AI?\n",
      " is the the science of science of production of the high of society. of production of the high of various phase. of various society. attack hypothetical bioinformatics sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun\n",
      "Are you sentient?\n",
      " i am a lot <end>\n",
      "Are you sentient?\n",
      " i am a lot <end>\n",
      "Are you sentient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "What language are you written in?\n",
      " i am a lot <end>\n"
     ]
    }
   ],
   "source": [
    "for sentence in questions[0:10]:\n",
    "    print(sentence)\n",
    "    padSentence = preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([sentence])[0]] , \n",
    "                                                       maxlen=maxlen_questions , padding='pre')\n",
    "    states_values = enc_model2.predict(padSentence)\n",
    "#     states_values = enc_model2.predict( str_to_tokens(sentence, maxlen_questions) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model2.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      " is the the science of science of production of the high of society. of production of the high of various phase. of various society. attack hypothetical bioinformatics sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun\n",
      "What is AI?\n",
      " is the the science of science of production of the high of society. of production of the high of various phase. of various society. attack hypothetical bioinformatics sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun sun\n",
      "Are you sentient?\n",
      " i am a lot <end>\n",
      "Are you sentient?\n",
      " i am a lot <end>\n",
      "Are you sentient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "Are you sapient?\n",
      " i am a lot <end>\n",
      "What language are you written in?\n",
      " i am a lot <end>\n"
     ]
    }
   ],
   "source": [
    "for sentence in questions[0:10]:\n",
    "    print(sentence)\n",
    "    padSentence = preprocessing.sequence.pad_sequences([tokenizer.texts_to_sequences([sentence])[0]] , \n",
    "                                                       maxlen=maxlen_questions , padding='pre')\n",
    "    statesValues = enc_model2.predict(padSentence)\n",
    "    inpTargetSeq = np.zeros( ( 1 , 1 ) )\n",
    "    inpTargetSeq[0, 0] = tokenizer.word_index['<start>']\n",
    "    reply = ''\n",
    "    while (1):\n",
    "        decOut , h , c = dec_model2.predict([ inpTargetSeq ] + statesValues )\n",
    "        predIndex = np.argmax(decOut[0][0])\n",
    "        predWord = tokenizer.index_word[predIndex]\n",
    "        reply += ' {}'.format(predWord)\n",
    "        \n",
    "        if predWord == '<end>' or len(reply.split()) > maxlen_answers:\n",
    "            break\n",
    "            \n",
    "        inpTargetSeq[ 0 , 0 ] = predIndex\n",
    "        statesValues = [ h , c ] \n",
    "\n",
    "    print( reply )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5109004e-03, 2.1564250e-09, 3.7139004e-01, ..., 3.6409412e-05,\n",
       "       3.9359611e-06, 2.0602038e-06], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_outputs[0, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, GRU, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "\n",
    "def define_nmt(hidden_size, batch_size, en_timesteps, en_vsize, fr_timesteps, fr_vsize):\n",
    "    \"\"\" Defining a NMT model \"\"\"\n",
    "\n",
    "    # Define an input sequence and process it.\n",
    "    if batch_size:\n",
    "        encoder_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(batch_shape=(batch_size, fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "    else:\n",
    "        encoder_inputs = Input(shape=(en_timesteps, en_vsize), name='encoder_inputs')\n",
    "        decoder_inputs = Input(shape=(fr_timesteps - 1, fr_vsize), name='decoder_inputs')\n",
    "\n",
    "    # Encoder GRU\n",
    "    encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')\n",
    "    encoder_out, encoder_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "    # Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "    decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')\n",
    "    decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "\n",
    "    # Attention layer\n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "    # Concat attention input and decoder GRU output\n",
    "    decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])\n",
    "\n",
    "    # Dense layer\n",
    "    dense = Dense(fr_vsize, activation='softmax', name='softmax_layer')\n",
    "    dense_time = TimeDistributed(dense, name='time_distributed_layer')\n",
    "    decoder_pred = dense_time(decoder_concat_input)\n",
    "\n",
    "    # Full model\n",
    "    full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "    full_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    full_model.summary()\n",
    "\n",
    "    \"\"\" Inference model \"\"\"\n",
    "    batch_size = 1\n",
    "\n",
    "    \"\"\" Encoder (Inference) model \"\"\"\n",
    "    encoder_inf_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inf_inputs')\n",
    "    encoder_inf_out, encoder_inf_state = encoder_gru(encoder_inf_inputs)\n",
    "    encoder_model = Model(inputs=encoder_inf_inputs, outputs=[encoder_inf_out, encoder_inf_state])\n",
    "\n",
    "    \"\"\" Decoder (Inference) model \"\"\"\n",
    "    decoder_inf_inputs = Input(batch_shape=(batch_size, 1, fr_vsize), name='decoder_word_inputs')\n",
    "    encoder_inf_states = Input(batch_shape=(batch_size, en_timesteps, hidden_size), name='encoder_inf_states')\n",
    "    decoder_init_state = Input(batch_shape=(batch_size, hidden_size), name='decoder_init')\n",
    "\n",
    "    decoder_inf_out, decoder_inf_state = decoder_gru(decoder_inf_inputs, initial_state=decoder_init_state)\n",
    "    attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "    decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_inf_out, attn_inf_out])\n",
    "    decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "    decoder_model = Model(inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "                          outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state])\n",
    "\n",
    "    return full_model, encoder_model, decoder_model\n",
    "\n",
    "def sents2sequences(tokenizer, sentences, reverse=False, pad_length=None, padding_type='post'):\n",
    "    encoded_text = tokenizer.texts_to_sequences(sentences)\n",
    "    preproc_text = preprocessing.sequence.pad_sequences(encoded_text, padding=padding_type, maxlen=pad_length)\n",
    "    if reverse:\n",
    "        preproc_text = np.flip(preproc_text, axis=1)\n",
    "\n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "from tensorflow.keras.backend import set_session\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "config_proto = tf.ConfigProto()\n",
    "off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "config_proto.graph_options.rewrite_options.arithmetic_optimization = off\n",
    "session = tf.Session(config=config_proto)\n",
    "set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(filters='\\t\\n', oov_token='UNK')\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "\n",
    "encoder_input_data = sents2sequences(tokenizer, questions, reverse=False, padding_type='pre', pad_length=maxlen_questions)\n",
    "decoder_input_data = sents2sequences(tokenizer, answers, pad_length=maxlen_answers)\n",
    "\n",
    "# padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "# encoder_input_data = np.array( padded_questions )\n",
    "\n",
    "# padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "# decoder_input_data = np.array( padded_answers )\n",
    "\n",
    "vsize = max(tokenizer.index_word.keys()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (64, 22, 2329)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (64, 73, 2329)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_gru (GRU)               [(64, 22, 96), (64,  698688      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_gru (GRU)               [(64, 73, 96), (64,  698688      decoder_inputs[0][0]             \n",
      "                                                                 encoder_gru[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer [(64, 73, 96), (64,  18528       encoder_gru[0][0]                \n",
      "                                                                 decoder_gru[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (64, 73, 192)        0           decoder_gru[0][0]                \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_layer (TimeDis (64, 73, 2329)       449497      concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,865,401\n",
      "Trainable params: 1,865,401\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model, infer_enc_model, infer_dec_model = define_nmt(\n",
    "        hidden_size=96, batch_size=64,\n",
    "        en_timesteps=maxlen_questions, fr_timesteps=maxlen_answers,\n",
    "        en_vsize=vsize, fr_vsize=vsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_onehot_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Loss in epoch 1: 7.637436032295227\n",
      "Loss in epoch 2: 7.2712976932525635\n",
      "Loss in epoch 3: 4.909414708614349\n",
      "Loss in epoch 4: 1.5088359713554382\n",
      "Loss in epoch 5: 1.5009416937828064\n",
      "Loss in epoch 6: 1.3769365176558495\n",
      "Loss in epoch 7: 1.3488154262304306\n",
      "Loss in epoch 8: 1.3201587498188019\n",
      "Loss in epoch 9: 1.2837055698037148\n",
      "Loss in epoch 10: 1.2385107427835464\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size =64\n",
    "for ep in range(n_epochs):\n",
    "    losses = []\n",
    "    for bi in range(0, encoder_input_data.shape[0] - batch_size, batch_size):\n",
    "\n",
    "        en_onehot_seq = utils.to_categorical(encoder_input_data[bi:bi + batch_size, :], num_classes=vsize)\n",
    "        fr_onehot_seq = utils.to_categorical(decoder_input_data[bi:bi + batch_size, :], num_classes=vsize)\n",
    "\n",
    "        full_model.train_on_batch([en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :])\n",
    "\n",
    "        l = full_model.evaluate([en_onehot_seq, fr_onehot_seq[:, :-1, :]], fr_onehot_seq[:, 1:, :],\n",
    "                                batch_size=batch_size, verbose=0)\n",
    "\n",
    "        losses.append(l)\n",
    "    if (ep + 1) % 1 == 0:\n",
    "        print(\"Loss in epoch {}: {}\".format(ep + 1, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is AI?\n",
      "\n",
      "What is AI?\n",
      "\n",
      "Are you sentient?\n",
      "\n",
      "Are you sentient?\n",
      "\n",
      "Are you sentient?\n",
      "\n",
      "Are you sapient?\n",
      "\n",
      "Are you sapient?\n",
      "\n",
      "Are you sapient?\n",
      "\n",
      "Are you sapient?\n",
      "\n",
      "What language are you written in?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentences in questions[0:10]:\n",
    "    print(sentences)\n",
    "    test_en_seq = sents2sequences(tokenizer, [sentences], pad_length=maxlen_questions)\n",
    "    test_fr_seq = sents2sequences(tokenizer, ['<start>'], vsize)\n",
    "    test_en_onehot_seq = utils.to_categorical(test_en_seq, num_classes=vsize)\n",
    "    test_fr_onehot_seq = np.expand_dims(utils.to_categorical(test_fr_seq, num_classes=vsize), 1)\n",
    "\n",
    "    enc_outs, enc_last_state = infer_enc_model.predict(test_en_onehot_seq)\n",
    "    dec_state = enc_last_state\n",
    "    attention_weights = []\n",
    "    fr_text = ''\n",
    "    for i in range(20):\n",
    "\n",
    "        dec_out, attention, dec_state = infer_dec_model.predict([enc_outs, dec_state, test_fr_onehot_seq])\n",
    "        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "\n",
    "        if dec_ind == 0:\n",
    "            break\n",
    "        fr_index2word = dict(zip(fr_tokenizer.word_index.values(), fr_tokenizer.word_index.keys()))\n",
    "        test_fr_seq = sents2sequences(fr_tokenizer, [fr_index2word[dec_ind]], vsize)\n",
    "        test_fr_onehot_seq = np.expand_dims(utils.to_categorical(test_fr_seq, num_classes=vsize), 1)\n",
    "\n",
    "        attention_weights.append((dec_ind, attention))\n",
    "        fr_text += fr_index2word[dec_ind] + ' '\n",
    "    print(fr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ChatBot_using_seq2seq.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
