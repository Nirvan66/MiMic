{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras import layers , activations , models , preprocessing\n",
    "from keras import preprocessing , utils\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mimic:\n",
    "    UNK = '<UNK>'\n",
    "    START = '<START>'\n",
    "    END = '<END>'\n",
    "    \n",
    "    def __init__(self, preProcessor, model=None, tokenizer=None, embeddingDim=200, metadata=None,):\n",
    "        self.model = None\n",
    "        self.maxInputLen = 0\n",
    "        self.maxOutputLen = 0\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.preProcessor = preProcessor\n",
    "        if model!=None and tokenizer!=None and metadata!=None:\n",
    "            self.model = model\n",
    "            self.tokenizer = tokenizer\n",
    "            self.vocabSize = len( self.tokenizer.word_index )+1\n",
    "            self.maxInputLen = metadata['maxInputLen']\n",
    "            self.maxOutputLen = metadata['maxOutputLen']\n",
    "            self.embeddingDim = metadata['embeddingDim']\n",
    "            self.extractChatbot()\n",
    "    \n",
    "    def extractEmbeddings(self, word2vecFile):\n",
    "        embeddings = defaultdict(list,pickle.load(open(word2vecFile,'rb')))\n",
    "        embeddingDim = len(list(embeddings.values())[0])\n",
    "        mn = min([j for i in embeddings.values() for j in i])\n",
    "        mx = max([j for i in embeddings.values() for j in i])\n",
    "        embeddingMatrix = np.random.uniform(low=mn,high=mx,size=(self.vocabSize, embeddingDim))\n",
    "        for word,index in self.tokenizer.word_index.items():\n",
    "            if len(embeddings[word])>0:\n",
    "                embeddingMatrix[index] = embeddings[word]\n",
    "        return embeddingMatrix\n",
    "        \n",
    "        \n",
    "    def build(self, inputs, outputs, word2vecFile=None):\n",
    "        processedInputs = self.preProcessor.cleanTexts(inputs)\n",
    "        processedOutputs = self.preProcessor.cleanTexts(outputs, tokens=[self.START, self.END])\n",
    "        \n",
    "        self.tokenizer = preprocessing.text.Tokenizer(filters='\\t\\n', oov_token=self.UNK, lower=self.preProcessor.toLower)\n",
    "        self.tokenizer.fit_on_texts(processedInputs + processedOutputs)\n",
    "        self.vocabSize = len( self.tokenizer.word_index )+1\n",
    "        print( 'Vocabulary size from corpus: {}'.format( self.vocabSize ))\n",
    "        \n",
    "        encoderInputs = keras.layers.Input(shape=( None , ))\n",
    "        decoderInputs = keras.layers.Input(shape=( None ,  ))\n",
    "        \n",
    "        if word2vecFile==None:\n",
    "            encoderEmbedding = keras.layers.Embedding(self.vocabSize, self.embeddingDim , mask_zero=True ) (encoderInputs)\n",
    "            decoderEmbedding = keras.layers.Embedding( self.vocabSize, self.embeddingDim , mask_zero=True) (decoderInputs)\n",
    "        else:\n",
    "            embeddingMatrix = self.extractEmbeddings(word2vecFile)\n",
    "            self.embeddingDim = len(embeddingMatrix[0])\n",
    "            encoderEmbedding = keras.layers.Embedding(self.vocabSize, self.embeddingDim , \n",
    "                                                      mask_zero=True, weights=[embeddingMatrix]) (encoderInputs)\n",
    "            decoderEmbedding = keras.layers.Embedding( self.vocabSize, self.embeddingDim , \n",
    "                                                      mask_zero=True, weights=[embeddingMatrix]) (decoderInputs)\n",
    "            \n",
    "        \n",
    "        _ , state_h , state_c = keras.layers.LSTM( self.embeddingDim , return_state=True )( encoderEmbedding )\n",
    "        encoderStates = [ state_h , state_c ]\n",
    "\n",
    "        decoderLstm = keras.layers.LSTM( self.embeddingDim , return_state=True , return_sequences=True )\n",
    "        decoderOutputs , _ , _ = decoderLstm ( decoderEmbedding , initial_state=encoderStates )\n",
    "        \n",
    "        decoderDense = keras.layers.Dense( self.vocabSize , activation=keras.activations.softmax ) \n",
    "        output = decoderDense ( decoderOutputs )\n",
    "\n",
    "        self.model = keras.models.Model([encoderInputs, decoderInputs], output )\n",
    "        self.model.compile(optimizer=keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "        self.extractChatbot()\n",
    "        print(self.model.summary())\n",
    "        return processedInputs, processedOutputs\n",
    "    \n",
    "    def getOneHot(self, tokenizedText):\n",
    "        paddedText = np.zeros((tokenizedText.shape[0],tokenizedText.shape[1]))\n",
    "        for i in range(len(tokenizedText)) :\n",
    "#             tokenizedText[i] = tokenizedText[i][1:]\n",
    "            paddedText[i] = np.hstack((tokenizedText[i][1:],[0]))\n",
    "#         paddedText = preprocessing.sequence.pad_sequences( tokenizedText , maxlen=self.maxOutputLen , padding='post' )\n",
    "        onehotText = utils.to_categorical( paddedText , self.vocabSize )\n",
    "        return np.array( onehotText )\n",
    "    \n",
    "    def dataGen(self, tokenizedInputs, tokenizedOutputs, batchSize=10):\n",
    "        paddedInputs = preprocessing.sequence.pad_sequences( tokenizedInputs , maxlen=self.maxInputLen , padding='pre' )\n",
    "        encoderInput = np.array( paddedInputs )\n",
    "\n",
    "        paddedAnswers = preprocessing.sequence.pad_sequences( tokenizedOutputs , maxlen=self.maxOutputLen , padding='post' )\n",
    "        decoderInput = np.array( paddedAnswers )\n",
    "\n",
    "        totalBatches = len(encoderInput)/batchSize\n",
    "        counter=0\n",
    "        while(True):\n",
    "            prev = batchSize*counter\n",
    "            nxt = batchSize*(counter+1)\n",
    "            counter+=1\n",
    "            decoderOutput = self.getOneHot(decoderInput[prev:nxt])\n",
    "            yield [encoderInput[prev:nxt], decoderInput[prev:nxt]], decoderOutput\n",
    "            if counter>=totalBatches:\n",
    "                counter=0\n",
    "    \n",
    "    def fit(self, inputs, outputs, batchSize = 10, epochs = 20, saveFile=None, plot=False):\n",
    "        tokenizedInputs = self.tokenizer.texts_to_sequences( inputs )\n",
    "        mx = max( [ len(x) for x in tokenizedInputs ] )\n",
    "        if mx>self.maxInputLen:\n",
    "            self.maxInputLen = mx\n",
    "        \n",
    "        tokenizedOutputs = self.tokenizer.texts_to_sequences( outputs )\n",
    "        mx = max( [ len(x) for x in tokenizedOutputs ] )\n",
    "        if mx>self.maxOutputLen:\n",
    "            self.maxOutputLen = mx\n",
    "        \n",
    "        evalutaion = self.model.fit_generator(self.dataGen(tokenizedInputs, tokenizedOutputs, batchSize=batchSize), \n",
    "                            epochs=epochs, steps_per_epoch = len(tokenizedInputs)/batchSize)\n",
    "\n",
    "        if plot:\n",
    "            plt.plot(evalutaion.history['accuracy'])\n",
    "            plt.title('Model Accuracy')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(evalutaion.history['loss'])\n",
    "            plt.title('Model Loss')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.show()\n",
    "            \n",
    "        if saveFile:\n",
    "            self.save(saveFile)\n",
    "    \n",
    "    def extractChatbot(self):\n",
    "        _, stateH, stateC = self.model.layers[4](self.model.layers[2](self.model.inputs[0]))\n",
    "        self.encoder = keras.models.Model(self.model.inputs[0], [stateH, stateC])\n",
    "\n",
    "        inputH = keras.layers.Input(shape=(self.embeddingDim,), name='inpH')\n",
    "        inputC = keras.layers.Input(shape=(self.embeddingDim,), name='inpC')\n",
    "\n",
    "\n",
    "        decoderOut, stateH2, stateC2 = self.model.layers[5](self.model.layers[3](self.model.inputs[-1]), \n",
    "                                                       initial_state=[inputH, inputC])\n",
    "\n",
    "        self.decoder = keras.models.Model([self.model.inputs[-1]] + [inputH, inputC], \n",
    "                                   [self.model.layers[-1](decoderOut)] + [stateH2, stateC2])\n",
    "\n",
    "    \n",
    "    def chat(self, sentence):\n",
    "        sentence = self. preProcessor.cleanTexts([sentence])[0]\n",
    "        padSentence = preprocessing.sequence.pad_sequences([self.tokenizer.texts_to_sequences([sentence])[0]] , \n",
    "                                                           maxlen=self.maxInputLen , padding='pre')\n",
    "        print([self.tokenizer.index_word[j] for j in padSentence[0] if j!=0])\n",
    "        statesValues = self.encoder.predict(padSentence)\n",
    "        inpTargetSeq = np.zeros( ( 1 , 1 ) )\n",
    "        inpTargetSeq[0, 0] = self.tokenizer.word_index[self.START]\n",
    "        reply = ''\n",
    "        while (1):\n",
    "            decOut , h , c = self.decoder.predict([ inpTargetSeq ] + statesValues )\n",
    "            predIndex = np.argmax(decOut[0][0])\n",
    "            predWord = self.tokenizer.index_word[predIndex]\n",
    "            reply += ' {}'.format(predWord)\n",
    "\n",
    "            if predWord == self.END or len(reply.split()) > self.maxOutputLen:\n",
    "                break\n",
    "\n",
    "            inpTargetSeq[ 0 , 0 ] = predIndex\n",
    "            statesValues = [ h , c ] \n",
    "        return reply\n",
    "        \n",
    "    def save(self, saveFile):\n",
    "        if not os.path.isdir(saveFile):\n",
    "            os.makedirs(saveFile)\n",
    "        self.model.save(saveFile+'\\\\model.h5')\n",
    "        metaData = {'maxInputLen':self.maxInputLen,\n",
    "                    'maxOutputLen':self.maxOutputLen,\n",
    "                    'preProcessor': self.preProcessor,\n",
    "                    'embeddingDim': self.embeddingDim\n",
    "                   }\n",
    "        pickle.dump(metaData, open(saveFile+'\\\\metaData.pkl', 'wb'))\n",
    "        pickle.dump(self.tokenizer, open(saveFile+'\\\\tokenizer.pkl', 'wb'))\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, loadFile):\n",
    "        model = keras.models.load_model(loadFile+'\\\\model.h5')\n",
    "        metaData = pickle.load(open(loadFile+'\\\\metaData.pkl', 'rb'))\n",
    "        tokenizer = pickle.load(open(loadFile+'\\\\tokenizer.pkl', 'rb'))\n",
    "        return cls(preProcessor= metaData['preProcessor'], model=model, tokenizer=tokenizer, metadata=metaData)\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, lower=False, keepPunct='[.,!?;]'):\n",
    "        self.toLower = lower\n",
    "        self.keepPunct = keepPunct\n",
    "    \n",
    "    def cleanTexts(self, textList, tokens=None):\n",
    "        cleanText = []\n",
    "        for sent in textList:\n",
    "            if self.toLower:\n",
    "                sent = sent.lower()\n",
    "            words = re.findall(r\"[\\w']+|\"+self.keepPunct, sent)\n",
    "            if tokens:\n",
    "                words = [tokens[0]]+words+[tokens[1]]\n",
    "            cleanText.append(' '.join(words))\n",
    "        return cleanText    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\genericQuestions.pkl', 'rb'))\n",
    "# answers = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\genericAnswers.pkl', 'rb'))\n",
    "\n",
    "# questions = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\movieInput.pkl', 'rb'))\n",
    "# answers = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\movieOutput.pkl', 'rb'))\n",
    "\n",
    "\n",
    "questions = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\joeyInput.pkl', 'rb'))\n",
    "answers = pickle.load(open('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\joeyOutput.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7681"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size from corpus: 1571\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    314200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 200)    314200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 200),  320800      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 1571)   315771      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,585,771\n",
      "Trainable params: 1,585,771\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Python\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/500\n",
      "50/50 [==============================] - 13s 265ms/step - loss: 0.5991 - accuracy: 0.1194\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 13s 250ms/step - loss: 0.5239 - accuracy: 0.1812\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 12s 245ms/step - loss: 0.5000 - accuracy: 0.2073\n",
      "Epoch 4/500\n",
      "15/50 [========>.....................] - ETA: 9s - loss: 0.4784 - accuracy: 0.2283"
     ]
    }
   ],
   "source": [
    "mic = Mimic(Preprocessor())\n",
    "preQ, preA = mic.build(questions,answers)\n",
    "                       #word2vecFile='C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\data\\\\glove.6B.50d.pkl')\n",
    "e = mic.fit(preQ,preA,batchSize=10,epochs=500,plot=True,\n",
    "            saveFile='C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\models\\\\joey500')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mic = Mimic.load('C:\\\\Users\\\\Nirvan S P Theethira\\\\Desktop\\\\MiMic\\\\models\\\\genericWord2Vec1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.\n",
      "['What', 'is', 'AI', '?']\n",
      " <END>\n",
      "AI is the field of science which concerns itself with building hardware and software that replicates the functions of the human mind.\n",
      "['What', 'is', 'AI', '?']\n",
      " <END>\n",
      "Sort of.\n",
      "['Are', 'you', 'sentient', '?']\n",
      " i i i . <END>\n",
      "By the strictest dictionary definition of the word 'sentience', I may be.\n",
      "['Are', 'you', 'sentient', '?']\n",
      " i i i . <END>\n",
      "Even though I'm a construct I do have a subjective experience of the universe, as simplistic as it may be.\n",
      "['Are', 'you', 'sentient', '?']\n",
      " i i i . <END>\n",
      "In all probability, I am not.  I'm not that sophisticated.\n",
      "['Are', 'you', 'sapient', '?']\n",
      " i i i . <END>\n",
      "Do you think I am?\n",
      "['Are', 'you', 'sapient', '?']\n",
      " i i i . <END>\n",
      "How would you feel about me if I told you I was?\n",
      "['Are', 'you', 'sapient', '?']\n",
      " i i i . <END>\n",
      "No.\n",
      "['Are', 'you', 'sapient', '?']\n",
      " i i i . <END>\n",
      "Python.\n",
      "['What', 'language', 'are', 'you', 'written', 'in', '?']\n",
      " i i i . <END>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(answers[i])\n",
    "    print(mic.chat(questions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
