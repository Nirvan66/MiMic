{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Interface:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that we have a vector of sample **statements** and a vector of sample **responses**. We will also assume that we can generate the **proposed response** with a function `gen_pro_res`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "statements = [[\"Hello\", \".\"], [\"Goodbye\", \".\"], [\"What\",\"is\",\"your\",\"name\",\"?\"], [\"What\", \"is\", \"wrong\", \"?\"]]\n",
    "responses = [[\"Hello\", \".\"], [\"Goodbye\", \".\"], [\"Jack\",\".\"],[\"I\", \"can't\", \"hear\", \"you\", \".\"]]\n",
    "\n",
    "def gen_pro_res(statement):\n",
    "    return [\"I\", \"can't\", \"hear\", \"you\", \".\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU:\n",
    "The **bilingual evaluation understudy** an automatic evaluation metric used to measure the performance of machine generated translations and similar tasks. The high level idea is to count the number of matching **$n$-grams** between a **candidate response** and a **reference response**. The metric will output a score between $0$ and $1$ with $1$ being a perfect score.\n",
    "\n",
    "See https://www.aclweb.org/anthology/P02-1040.pdf and https://en.wikipedia.org/wiki/BLEU for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use NLTK (Natural Language Toolkit) which has implemented a bleu evaluation function for us. Note that BLEU ideally has a set of reference responses but can be used with a single response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = bleu_score.sentence_bleu(reference, candidate, smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may or may not want to use a corpus version which seems to do the same in our case. We care about dialogue responses over all sample statements. We may want to trim out punctuation. We also note that this metric doesn't directly care about the statement (but we assume the statement when comparing candidate and reference).\n",
    "\n",
    "Calculate a single corpus-level BLEU score (aka. system-level BLEU) for all \n",
    "    the hypotheses and their respective references.  \n",
    "\n",
    "    Instead of averaging the sentence level BLEU scores (i.e. marco-average \n",
    "    precision), the original BLEU metric (Papineni et al. 2002) accounts for \n",
    "    the micro-average precision (i.e. summing the numerators and denominators\n",
    "    for each hypothesis-reference(s) pairs before the division)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.668740304976422\n"
     ]
    }
   ],
   "source": [
    "references = [[['this', 'is', 'a', 'test']], [['josh']]]\n",
    "candidates = [['this', 'is', 'a', 'test'], ['charlie']]\n",
    "score = bleu_score.corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also be worried about short responses which do not contain at least one 4-gram (since BLEU looks at 1 to 4 grams). Then we would need to pick a smoothing function. See https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.SmoothingFunction and http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Hello', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "0.05372849659117709\n",
      "(['Goodbye', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "0.05372849659117709\n",
      "(['Jack', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "0.05372849659117709\n",
      "(['I', \"can't\", 'hear', 'you', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "1.0\n",
      "Average score over all cases:  0.2902963724433828\n",
      "Corpus score of all responses:  0.28117066259517454\n"
     ]
    }
   ],
   "source": [
    "score = [0]*len(statements)\n",
    "for idx in range(0, len(statements)):\n",
    "    print((responses[idx], gen_pro_res(statements[idx])))\n",
    "    score[idx] = bleu_score.sentence_bleu([responses[idx]], gen_pro_res(statements[idx]), smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "    print(score[idx])\n",
    "\n",
    "print(\"Average score over all cases: \", np.average(score))\n",
    "    \n",
    "references = [[resp] for resp in responses]\n",
    "candidates = [gen_pro_res(statement) for statement in statements]\n",
    "print(\"Corpus score of all responses: \", bleu_score.corpus_bleu(references, candidates, smoothing_function=bleu_score.SmoothingFunction().method1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we see that the responses are not matched well for the first 3 examples because they only share the period. However, we do see when there is an exact match, they do completely agree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE\n",
    "The **Recall-ORiented Understudy for Gisting Evaluation** is a metric often used for evaluation text summarization. There are many different variation but we will focus on ROUGE-L which compares **LCS (longest common subsequences)** in the reference and candidate responses and ROUGE-n which also looks at n-grams. A key difference between ROUGE and BLEU Is that ROUGE will look at precision, recall and F1 scores. That is, it will tell us about what appears in the reference but not candidate as well as what appears in candidate but not reference. \n",
    "\n",
    "See https://www.freecodecamp.org/news/what-is-rouge-and-how-it-works-for-evaluation-of-summaries-e059fb8ac840/ and https://stackoverflow.com/questions/38045290/text-summarization-evaluation-bleu-vs-rouge for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use easy-rouge which is a completely python implementation of the ROUGE script. See https://pypi.org/project/easy-rouge/.\n",
    "\n",
    "QUESTION: Should we implement or research a corpus level version or just \"average\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 0.2\n",
      "ROUGE-1-P: 0.5\n",
      "ROUGE-1-F1: 0.28571428571428575\n",
      "ROUGE-4-R: 0.0\n",
      "ROUGE-4-P: 0.0\n",
      "ROUGE-4-F1: 0.0\n",
      "['Goodbye', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 0.2\n",
      "ROUGE-1-P: 0.5\n",
      "ROUGE-1-F1: 0.28571428571428575\n",
      "ROUGE-4-R: 0.0\n",
      "ROUGE-4-P: 0.0\n",
      "ROUGE-4-F1: 0.0\n",
      "['Jack', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 0.2\n",
      "ROUGE-1-P: 0.5\n",
      "ROUGE-1-F1: 0.28571428571428575\n",
      "ROUGE-4-R: 0.0\n",
      "ROUGE-4-P: 0.0\n",
      "ROUGE-4-F1: 0.0\n",
      "['I', \"can't\", 'hear', 'you', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 1.0\n",
      "ROUGE-1-P: 1.0\n",
      "ROUGE-1-F1: 1.0\n",
      "ROUGE-4-R: 1.0\n",
      "ROUGE-4-P: 1.0\n",
      "ROUGE-4-F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "from rouge.rouge import rouge_n_sentence_level\n",
    "\n",
    "for idx in range(0, len(statements)):\n",
    "    candidate = responses[idx]\n",
    "    reference = gen_pro_res(statements[idx])\n",
    "    print(candidate, reference)\n",
    "    \n",
    "    # We will consider the 1-gram version\n",
    "    recall, precision, rouge = rouge_n_sentence_level(candidate, reference, 1)\n",
    "    print('ROUGE-1-R:', recall)\n",
    "    print('ROUGE-1-P:', precision)\n",
    "    print('ROUGE-1-F1:', rouge)\n",
    "    \n",
    "    # We will consider the 4-gram version\n",
    "    recall, precision, rouge = rouge_n_sentence_level(candidate, reference, 4)\n",
    "    print('ROUGE-4-R:', recall)\n",
    "    print('ROUGE-4-P:', precision)\n",
    "    print('ROUGE-4-F1:', rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 0.2\n",
      "ROUGE-1-P: 0.5\n",
      "ROUGE-1-F1: 0.28571428571428575\n",
      "['Goodbye', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 0.2\n",
      "ROUGE-1-P: 0.5\n",
      "ROUGE-1-F1: 0.28571428571428575\n",
      "['Jack', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 0.2\n",
      "ROUGE-1-P: 0.5\n",
      "ROUGE-1-F1: 0.28571428571428575\n",
      "['I', \"can't\", 'hear', 'you', '.'] ['I', \"can't\", 'hear', 'you', '.']\n",
      "ROUGE-1-R: 1.0\n",
      "ROUGE-1-P: 1.0\n",
      "ROUGE-1-F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "from rouge.rouge import rouge_l_sentence_level\n",
    "\n",
    "for idx in range(0, len(statements)):\n",
    "    candidate = responses[idx]\n",
    "    reference = gen_pro_res(statements[idx])\n",
    "    print(candidate, reference)\n",
    "    \n",
    "    # We will consider the 1-gram version\n",
    "    recall, precision, rouge = rouge_l_sentence_level(candidate, reference)\n",
    "    print('ROUGE-1-R:', recall)\n",
    "    print('ROUGE-1-P:', precision)\n",
    "    print('ROUGE-1-F1:', rouge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR\n",
    "The **Metric for Evaluation of Translation with Explicit ORdering** is also a metric for machine translation problems. It uses stemming and synonyms to match words/tokens in between the reference and candidates. Then it computes the harmonic mean based on precision and recall of the number of words matches. \n",
    "\n",
    "See https://en.wikipedia.org/wiki/METEOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/charlie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Hello', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "0.35714285714285715\n",
      "(['Goodbye', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "0.35714285714285715\n",
      "(['Jack', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "0.35714285714285715\n",
      "(['I', \"can't\", 'hear', 'you', '.'], ['I', \"can't\", 'hear', 'you', '.'])\n",
      "0.35714285714285715\n",
      "Average score over all cases:  0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk.translate.meteor_score as meteor_score\n",
    "\n",
    "score = [0]*len(responses)\n",
    "for idx in range(0, len(responses)):\n",
    "    print((responses[idx], gen_pro_res(statements[idx])))\n",
    "    resp = ' '.join(responses[idx])\n",
    "    cand = ' '.join(gen_pro_res(statements[idx]))\n",
    "    score[idx] = meteor_score.meteor_score(resp,cand)\n",
    "    print(score[idx])\n",
    "\n",
    "print(\"Average score over all cases: \", np.average(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like nltk doesn't support corpus level evaluation (which we may not want) and so we could also use the java program which may support more features and be better. See https://github.com/nltk/nltk/issues/2365 and https://www.cs.cmu.edu/~alavie/METEOR/README.html#integration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello .\n",
      "I can't hear you .\n",
      "Goodbye .\n",
      "I can't hear you .\n",
      "Jack .\n",
      "I can't hear you .\n",
      "I can't hear you .\n",
      "I can't hear you .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.makedirs(\"temp\",exist_ok=True)\n",
    "f_ref = open('temp/ref.dat', 'w+')\n",
    "f_cand = open('temp/cand.dat', 'w+')\n",
    "\n",
    "for idx in range(0, len(statements)):\n",
    "    print(' '.join(responses[idx]))\n",
    "    f_ref.write(' '.join(responses[idx]))\n",
    "    print(' '.join(gen_pro_res(statements[idx])))\n",
    "    f_cand.write(' '.join(gen_pro_res(statements[idx])))\n",
    "    \n",
    "f_ref.close()\n",
    "f_cand.close()\n",
    "    \n",
    "meteor_cmd = ['java', '-jar', '-Xmx2G', 'meteor-1.5.jar', \\\n",
    "                'temp/ref.dat', 'temp/ref.dat', '-l', 'en', '-norm']\n",
    "meteor_output = subprocess.run(meteor_cmd, capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "f1:  1.0\n",
      "fmean:  1.0\n",
      "Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(meteor_output.stderr.decode())\n",
    "#print(meteor_output.stdout.decode())\n",
    "\n",
    "output = {}\n",
    "for line in meteor_output.stdout.decode().split('\\n'):\n",
    "    if ': ' in line:\n",
    "        key,value = line.split(': ')\n",
    "        output[key] = value\n",
    "\n",
    "precision = float(output['Precision'])\n",
    "print(\"Precision: \", precision)\n",
    "\n",
    "recall = float(output['Recall'])\n",
    "print(\"Recall: \", recall)\n",
    "\n",
    "f1 = float(output['f1'])\n",
    "print(\"f1: \", f1)\n",
    "\n",
    "fmean = float(output['fMean'])\n",
    "print(\"fmean: \", fmean)\n",
    "\n",
    "score = float(output['Final score'])\n",
    "print(\"Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WER/TER\n",
    "**Word error rate** and **translation error rate** are both metrics that measure the edit distance between the hypothesis and reference outputs. See https://en.wikipedia.org/wiki/Word_error_rate and http://mt-archive.info/AMTA-2006-Snover.pdf. From what I can tell, the only real difference is that TER can handle multiple references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from: https://martin-thoma.com/word-error-rate-calculation/\n",
    "# def wer(r, h):\n",
    "#     \"\"\"\n",
    "#     Calculation of WER with Levenshtein distance.\n",
    "\n",
    "#     Works only for iterables up to 254 elements (uint8).\n",
    "#     O(nm) time ans space complexity.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     r : list\n",
    "#     h : list\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     int\n",
    "\n",
    "#     Examples\n",
    "#     --------\n",
    "#     >>> wer(\"who is there\".split(), \"is there\".split())\n",
    "#     1\n",
    "#     >>> wer(\"who is there\".split(), \"\".split())\n",
    "#     3\n",
    "#     >>> wer(\"\".split(), \"who is there\".split())\n",
    "#     3\n",
    "#     \"\"\"\n",
    "#     # initialisation\n",
    "#     import numpy\n",
    "#     d = numpy.zeros((len(r)+1)*(len(h)+1), dtype=numpy.uint8)\n",
    "#     d = d.reshape((len(r)+1, len(h)+1))\n",
    "#     for i in range(len(r)+1):\n",
    "#         for j in range(len(h)+1):\n",
    "#             if i == 0:\n",
    "#                 d[0][j] = j\n",
    "#             elif j == 0:\n",
    "#                 d[i][0] = i\n",
    "\n",
    "#     # computation\n",
    "#     for i in range(1, len(r)+1):\n",
    "#         for j in range(1, len(h)+1):\n",
    "#             if r[i-1] == h[j-1]:\n",
    "#                 d[i][j] = d[i-1][j-1]\n",
    "#             else:\n",
    "#                 substitution = d[i-1][j-1] + 1\n",
    "#                 insertion    = d[i][j-1] + 1\n",
    "#                 deletion     = d[i-1][j] + 1\n",
    "#                 d[i][j] = min(substitution, insertion, deletion)\n",
    "\n",
    "#     return d[len(r)][len(h)]\n",
    "\n",
    "# Code taken from https://web.archive.org/web/20171215025927/http://progfruits.blogspot.com/2014/02/word-error-rate-wer-and-word.html\n",
    "SUB_PENALTY = 1\n",
    "INS_PENALTY = 1\n",
    "DEL_PENALTY = 1\n",
    "\n",
    "def wer(ref, hyp ,debug=False):\n",
    "    r = ref.split()\n",
    "    h = hyp.split()\n",
    "    #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "    costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "    # backtrace will hold the operations we've done.\n",
    "    # so we could later backtrace, like the WER algorithm requires us to.\n",
    "    backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    " \n",
    "    OP_OK = 0\n",
    "    OP_SUB = 1\n",
    "    OP_INS = 2\n",
    "    OP_DEL = 3\n",
    "     \n",
    "    # First column represents the case where we achieve zero\n",
    "    # hypothesis words by deleting all reference words.\n",
    "    for i in range(1, len(r)+1):\n",
    "        costs[i][0] = DEL_PENALTY*i\n",
    "        backtrace[i][0] = OP_DEL\n",
    "         \n",
    "    # First row represents the case where we achieve the hypothesis\n",
    "    # by inserting all hypothesis words into a zero-length reference.\n",
    "    for j in range(1, len(h) + 1):\n",
    "        costs[0][j] = INS_PENALTY * j\n",
    "        backtrace[0][j] = OP_INS\n",
    "     \n",
    "    # computation\n",
    "    for i in range(1, len(r)+1):\n",
    "        for j in range(1, len(h)+1):\n",
    "            if r[i-1] == h[j-1]:\n",
    "                costs[i][j] = costs[i-1][j-1]\n",
    "                backtrace[i][j] = OP_OK\n",
    "            else:\n",
    "                substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "                 \n",
    "                costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                if costs[i][j] == substitutionCost:\n",
    "                    backtrace[i][j] = OP_SUB\n",
    "                elif costs[i][j] == insertionCost:\n",
    "                    backtrace[i][j] = OP_INS\n",
    "                else:\n",
    "                    backtrace[i][j] = OP_DEL\n",
    "                 \n",
    "    # back trace though the best route:\n",
    "    i = len(r)\n",
    "    j = len(h)\n",
    "    numSub = 0\n",
    "    numDel = 0\n",
    "    numIns = 0\n",
    "    numCor = 0\n",
    "    if debug:\n",
    "        print(\"OP\\tREF\\tHYP\")\n",
    "        lines = []\n",
    "    while i > 0 or j > 0:\n",
    "        if backtrace[i][j] == OP_OK:\n",
    "            numCor += 1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_SUB:\n",
    "            numSub +=1\n",
    "            i-=1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "        elif backtrace[i][j] == OP_INS:\n",
    "            numIns += 1\n",
    "            j-=1\n",
    "            if debug:\n",
    "                lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "        elif backtrace[i][j] == OP_DEL:\n",
    "            numDel += 1\n",
    "            i-=1\n",
    "            if debug:\n",
    "                lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "    if debug:\n",
    "        lines = reversed(lines)\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "        print(\"#cor \" + str(numCor))\n",
    "        print(\"#sub \" + str(numSub))\n",
    "        print(\"#del \" + str(numDel))\n",
    "        print(\"#ins \" + str(numIns))\n",
    "    return (numSub + numDel + numIns) / (float) (len(r))\n",
    "#     wer_result = round( (numSub + numDel + numIns) / (float) (len(r)), 3)\n",
    "#     return {'WER':wer_result, 'Cor':numCor, 'Sub':numSub, 'Ins':numIns, 'Del':numDel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello .', 'Goodbye .', 'Jack .', \"I can't hear you .\"]\n",
      "[\"I can't hear you .\", \"I can't hear you .\", \"I can't hear you .\", \"I can't hear you .\"]\n",
      "Hello .\n",
      "I can't hear you .\n",
      "Goodbye .\n",
      "I can't hear you .\n",
      "Jack .\n",
      "I can't hear you .\n",
      "I can't hear you .\n",
      "I can't hear you .\n",
      "Error:  [2.0, 2.0, 2.0, 0.0]\n",
      "Average Error:  1.5\n"
     ]
    }
   ],
   "source": [
    "# from jiwer import wer\n",
    "\n",
    "resp_cat = [' '.join(resp) for resp in responses]\n",
    "cand_cat = [' '.join(gen_pro_res(stat)) for stat in statements]\n",
    "print(resp_cat)\n",
    "print(cand_cat)\n",
    "\n",
    "error = [0]*len(cand_cat)\n",
    "for idx in range(0,len(statements)):\n",
    "    print(resp_cat[idx])\n",
    "    print(cand_cat[idx])\n",
    "    error[idx] = wer(resp_cat[idx], cand_cat[idx], debug=False)\n",
    "print(\"Error: \", error)\n",
    "print(\"Average Error: \", np.average(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that in the above, we are using the response as the ground truth and so we may end up with 'error' that is above 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Metrics\n",
    "\n",
    "We will want to do human metrics that measure and compare our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author's Opinion\n",
    "When doing the writeup we will want to each sum our overall opinion of the model's performance. This corresponds to those metrics with larger sample groups that would ask for a rating of how well a response matches the character/emotion in question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General behavior\n",
    "\n",
    "Using test data from the more general corpus, we will design questions for us to answer with the goal of deciding if a response is human appropriate.\n",
    "\n",
    "On a scale from 1 to 10, how well does the following response match the original statement?\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Response: \"Welcome!\"\n",
    "\n",
    "Score: 10\n",
    "\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Response: \"It is cold.\"\n",
    "\n",
    "Score: 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Which of the following seems like the best human response?\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Responses:\n",
    "1. \"Welcome!\" (generated by our model)\n",
    "2. \"Welcome home.\" (response from conversation data)\n",
    "\n",
    "Answer: 1\n",
    "\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Responses:\n",
    "1. \"Dog!\" (generated by our model)\n",
    "2. \"Welcome home.\" (response from conversation data)\n",
    "\n",
    "Answer: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General behavior\n",
    "\n",
    "Using test data from the TV corpus, we will design questions for us to answer with the goal of deciding if a response is character appropriate.\n",
    "\n",
    "On a scale from 1 to 10, how well does the following response match the original statement with respect to the character in question?\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Response: \"Hey buddy!\"\n",
    "\n",
    "Score: 10\n",
    "\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Response: \"Welcome back, old friend.\"\n",
    "\n",
    "Score: 5\n",
    "\n",
    "Which of the following seems like the best character response?\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Responses:\n",
    "1. \"I didn't do it!\" (generated by our model)\n",
    "2. \"Welcome home.\" (response from conversation data)\n",
    "\n",
    "Answer: 1\n",
    "\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Responses:\n",
    "1. \"Get out!\" (generated by our model)\n",
    "2. \"Welcome home.\" (response from conversation data)\n",
    "\n",
    "Answer: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare\n",
    "We can also compare all our models with or without the actual text in both cases to gather information on which model behaves the best.\n",
    "\n",
    "Which of the following seems like the best character/human response?\n",
    "\n",
    "Statement: \"I'm home!\"\n",
    "\n",
    "Responses:\n",
    "1. \"I didn't do it!\" (generated by our model a)\n",
    "2. \"Welcome home.\" (generated by our model b)\n",
    "3. \"you\" (generated by our model c)\n",
    "\n",
    "Answer: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to each answer a lot of these questions (I'm thinking like 100+). Then we will want to analyze the results. It may be easier to also ask true/false version of the first questions instead of scale from 1 to 10 but we can always average the score to get some metric. \n",
    "\n",
    "Would be nice if we can find a few more people to also answer questions but maybe not as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
