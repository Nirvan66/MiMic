{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu_score\n",
    "from rouge.rouge import rouge_n_sentence_level\n",
    "from rouge.rouge import rouge_l_sentence_level\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "class AutomaticMetricTester:\n",
    "    def __init__(self, model, statements, references, tempDataFolder='temp/', sample=True, sample_size=10):\n",
    "        self._model = model\n",
    "        pairs = list(zip(statements, references))\n",
    "        random.shuffle(pairs)\n",
    "        self._statements = [pair[0] for pair in pairs[0:min(sample_size, len(pairs))]]\n",
    "        self._references = [pair[1] for pair in pairs[0:min(sample_size, len(pairs))]]\n",
    "        self._candidates = self.breakTexts([self._model.chat(text) for text in self._statements])\n",
    "        self._tempDataFolder = tempDataFolder\n",
    "        \n",
    "        self._blue_score = None\n",
    "        \n",
    "        self._rouge_1_recall = []\n",
    "        self._rouge_1_precision = []\n",
    "        self._rouge_1_f1 = []\n",
    "\n",
    "        self._rouge_4_recall = []\n",
    "        self._rouge_4_precision = []\n",
    "        self._rouge_4_f1 = []\n",
    "\n",
    "        self._rouge_el_recall = []\n",
    "        self._rouge_el_precision = []\n",
    "        self._rouge_el_f1 = []\n",
    "        \n",
    "        self._meteor_precision = None\n",
    "        self._meteor_recall = None\n",
    "        self._meteor_f1 = None\n",
    "        self._meteor_fmean = None\n",
    "        self._meteor_score = None\n",
    "        \n",
    "        self._wer_error = None\n",
    "        \n",
    "    # TODO: Move to preproc\n",
    "    def breakTexts(self, textList):\n",
    "        return [text.split(' ') for text in self._model.preProcessor.cleanTexts(textList)]\n",
    "    \n",
    "    def packTexts(self, textList):\n",
    "        return [[text] for text in textList]\n",
    "    \n",
    "    def unBreakTexts(self, textList):\n",
    "        return [' '.join(text) for text in textList]\n",
    "    \n",
    "    def computeBLUEScore(self):\n",
    "        blue_references = self.packTexts(self.breakTexts(self._references))\n",
    "        blue_candidates = self._candidates #self.breakTexts(self._candidates)\n",
    "        #print(blue_references)\n",
    "        #print(blue_candidates)\n",
    "        self._blue_score = bleu_score.corpus_bleu(blue_references, blue_candidates, smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "        \n",
    "    def computeROGUEScores(self):\n",
    "        self._rouge_1_recall = []\n",
    "        self._rouge_1_precision = []\n",
    "        self._rouge_1_f1 = []\n",
    "\n",
    "        self._rouge_4_recall = []\n",
    "        self._rouge_4_precision = []\n",
    "        self._rouge_4_f1 = []\n",
    "\n",
    "        self._rouge_el_recall = []\n",
    "        self._rouge_el_precision = []\n",
    "        self._rouge_el_f1 = []\n",
    "\n",
    "        for (candidate, reference) in zip(self._candidates, self._references):\n",
    "            #print(candidate, reference)\n",
    "\n",
    "            # We will consider the 1-gram version\n",
    "            recall, precision, rouge = rouge_n_sentence_level(candidate, reference, 1)\n",
    "            self._rouge_1_recall.append(recall)\n",
    "            self._rouge_1_precision.append(precision)\n",
    "            self._rouge_1_f1.append(rouge)\n",
    "\n",
    "            # We will consider the 4-gram version\n",
    "            recall, precision, rouge = rouge_n_sentence_level(candidate, reference, 4)\n",
    "            self._rouge_4_recall.append(recall)\n",
    "            self._rouge_4_precision.append(precision)\n",
    "            self._rouge_4_f1.append(rouge)\n",
    "\n",
    "            # We will consider the l version\n",
    "            recall, precision, rouge = rouge_l_sentence_level(candidate, reference)\n",
    "            self._rouge_el_recall.append(recall)\n",
    "            self._rouge_el_precision.append(precision)\n",
    "            self._rouge_el_f1.append(rouge)\n",
    "        \n",
    "    def computeMETEORScore(self):\n",
    "        os.makedirs(self._tempDataFolder, exist_ok=True)\n",
    "        \n",
    "        f_ref = open(self._tempDataFolder + '/ref.dat', 'w+')\n",
    "        f_cand = open(self._tempDataFolder + '/cand.dat', 'w+')\n",
    "\n",
    "        met_candidates = self.unBreakTexts(self._candidates)\n",
    "        for (reference, candidate) in zip(self._references, met_candidates):\n",
    "            f_ref.write(reference)\n",
    "            f_cand.write(candidate)\n",
    "\n",
    "        f_ref.close()\n",
    "        f_cand.close()\n",
    "\n",
    "        meteor_cmd = ['java', '-jar', '-Xmx2G', 'meteor-1.5.jar', \\\n",
    "                        'temp/ref.dat', 'temp/cand.dat', '-l', 'en', '-norm']\n",
    "        meteor_output = subprocess.run(meteor_cmd, capture_output=True)\n",
    "            \n",
    "        if meteor_output.stderr.decode() != \"\":\n",
    "            print(\"Error occured when running meteor tests!\")\n",
    "            print(meteor_output.stderr.decode())\n",
    "        else:\n",
    "            output = {}\n",
    "            for line in meteor_output.stdout.decode().split('\\n'):\n",
    "                if ': ' in line:\n",
    "                    key,value = line.split(': ')\n",
    "                    output[key] = value\n",
    "\n",
    "            self._meteor_precision = float(output['Precision'])\n",
    "            self._meteor_recall = float(output['Recall'])\n",
    "            self._meteor_f1 = float(output['f1'])\n",
    "            self._meteor_fmean = float(output['fMean'])\n",
    "            self._meteor_score = float(output['Final score'])\n",
    "            \n",
    "    # Code taken from https://web.archive.org/web/20171215025927/http://progfruits.blogspot.com/2014/02/word-error-rate-wer-and-word.html\n",
    "    def wer(self, references, candidates ,debug=False):\n",
    "        r = references.split()\n",
    "        h = candidates.split()\n",
    "\n",
    "        #costs will holds the costs, like in the Levenshtein distance algorithm\n",
    "        costs = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "        # backtrace will hold the operations we've done.\n",
    "        # so we could later backtrace, like the WER algorithm requires us to.\n",
    "        backtrace = [[0 for inner in range(len(h)+1)] for outer in range(len(r)+1)]\n",
    "\n",
    "        OP_OK = 0\n",
    "        OP_SUB = 1\n",
    "        OP_INS = 2\n",
    "        OP_DEL = 3\n",
    "\n",
    "        SUB_PENALTY = 1\n",
    "        INS_PENALTY = 1\n",
    "        DEL_PENALTY = 1\n",
    "\n",
    "        # First column represents the case where we achieve zero\n",
    "        # hypothesis words by deleting all reference words.\n",
    "        for i in range(1, len(r)+1):\n",
    "            costs[i][0] = DEL_PENALTY*i\n",
    "            backtrace[i][0] = OP_DEL\n",
    "\n",
    "        # First row represents the case where we achieve the hypothesis\n",
    "        # by inserting all hypothesis words into a zero-length reference.\n",
    "        for j in range(1, len(h) + 1):\n",
    "            costs[0][j] = INS_PENALTY * j\n",
    "            backtrace[0][j] = OP_INS\n",
    "\n",
    "        # computation\n",
    "        for i in range(1, len(r)+1):\n",
    "            for j in range(1, len(h)+1):\n",
    "                if r[i-1] == h[j-1]:\n",
    "                    costs[i][j] = costs[i-1][j-1]\n",
    "                    backtrace[i][j] = OP_OK\n",
    "                else:\n",
    "                    substitutionCost = costs[i-1][j-1] + SUB_PENALTY # penalty is always 1\n",
    "                    insertionCost    = costs[i][j-1] + INS_PENALTY   # penalty is always 1\n",
    "                    deletionCost     = costs[i-1][j] + DEL_PENALTY   # penalty is always 1\n",
    "\n",
    "                    costs[i][j] = min(substitutionCost, insertionCost, deletionCost)\n",
    "                    if costs[i][j] == substitutionCost:\n",
    "                        backtrace[i][j] = OP_SUB\n",
    "                    elif costs[i][j] == insertionCost:\n",
    "                        backtrace[i][j] = OP_INS\n",
    "                    else:\n",
    "                        backtrace[i][j] = OP_DEL\n",
    "\n",
    "        # back trace though the best route:\n",
    "        i = len(r)\n",
    "        j = len(h)\n",
    "        numSub = 0\n",
    "        numDel = 0\n",
    "        numIns = 0\n",
    "        numCor = 0\n",
    "        if debug:\n",
    "            print(\"OP\\tREF\\tHYP\")\n",
    "            lines = []\n",
    "        while i > 0 or j > 0:\n",
    "            if backtrace[i][j] == OP_OK:\n",
    "                numCor += 1\n",
    "                i-=1\n",
    "                j-=1\n",
    "                if debug:\n",
    "                    lines.append(\"OK\\t\" + r[i]+\"\\t\"+h[j])\n",
    "            elif backtrace[i][j] == OP_SUB:\n",
    "                numSub +=1\n",
    "                i-=1\n",
    "                j-=1\n",
    "                if debug:\n",
    "                    lines.append(\"SUB\\t\" + r[i]+\"\\t\"+h[j])\n",
    "            elif backtrace[i][j] == OP_INS:\n",
    "                numIns += 1\n",
    "                j-=1\n",
    "                if debug:\n",
    "                    lines.append(\"INS\\t\" + \"****\" + \"\\t\" + h[j])\n",
    "            elif backtrace[i][j] == OP_DEL:\n",
    "                numDel += 1\n",
    "                i-=1\n",
    "                if debug:\n",
    "                    lines.append(\"DEL\\t\" + r[i]+\"\\t\"+\"****\")\n",
    "        if debug:\n",
    "            lines = reversed(lines)\n",
    "            for line in lines:\n",
    "                print(line)\n",
    "            print(\"#cor \" + str(numCor))\n",
    "            print(\"#sub \" + str(numSub))\n",
    "            print(\"#del \" + str(numDel))\n",
    "            print(\"#ins \" + str(numIns))\n",
    "        return (numSub + numDel + numIns) / (float) (len(r))\n",
    "\n",
    "    def computeWER(self):\n",
    "        wer_candidates = self.unBreakTexts(self._candidates)\n",
    "\n",
    "        self._wer_error = []\n",
    "        for (reference,candidate) in zip(self._references, wer_candidates):\n",
    "            self._wer_error.append(self.wer(reference, candidate, debug=False))\n",
    "\n",
    "    def compileScores(self):\n",
    "        self.computeBLUEScore()\n",
    "        self.computeROGUEScores()\n",
    "        self.computeMETEORScore()\n",
    "        self.computeWER()\n",
    "        \n",
    "    def printScores(self):\n",
    "        print(\"BLUE SCORE: \", self._blue_score)\n",
    "        \n",
    "        print(\"ROGUE 1 Recall Average: \", np.average(self._rouge_1_recall))\n",
    "        print(\"ROGUE 1 Precision Average: \", np.average(self._rouge_1_precision))\n",
    "        print(\"ROGUE 1 F1: \", np.average(self._rouge_1_f1))\n",
    "        print(\"ROGUE 4 Recall Average: \", np.average(self._rouge_4_recall))\n",
    "        print(\"ROGUE 4 Precision Average: \", np.average(self._rouge_4_precision))\n",
    "        print(\"ROGUE 4 F1: \", np.average(self._rouge_4_f1))\n",
    "        print(\"ROGUE el Recall Average: \", np.average(self._rouge_el_recall))\n",
    "        print(\"ROGUE el Precision Average: \", np.average(self._rouge_el_precision))\n",
    "        print(\"ROGUE el F1: \", np.average(self._rouge_el_f1))\n",
    "        \n",
    "        print(\"METEOR Precision: \", self._meteor_precision)\n",
    "        print(\"METEOR Recall: \", self._meteor_recall)\n",
    "        print(\"METEOR f1: \", self._meteor_f1)\n",
    "        print(\"METEOR fmean: \", self._meteor_fmean)\n",
    "        print(\"METEOR Score: \", self._meteor_score)\n",
    "\n",
    "        print(\"WER Average Error: \", np.average(self._wer_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statements = pickle.load(open('../data/genericQuestionsTest.pkl', 'rb'))\n",
    "# references = pickle.load(open('../data/genericAnswersTest.pkl', 'rb'))\n",
    "\n",
    "# class Preprocessor:\n",
    "#     def __init__(self, lower=False, keepPunct='[.,!?;]'):\n",
    "#         self.toLower = lower\n",
    "#         self.keepPunct = keepPunct\n",
    "    \n",
    "#     def cleanTexts(self, textList, tokens=None):\n",
    "#         cleanText = []\n",
    "#         for sent in textList:\n",
    "#             if self.toLower:\n",
    "#                 sent = sent.lower()\n",
    "#             words = re.findall(r\"[\\w']+|\"+self.keepPunct, sent)\n",
    "#             if tokens:\n",
    "#                 words = [tokens[0]]+words+[tokens[1]]\n",
    "#             cleanText.append(' '.join(words))\n",
    "#         return cleanText \n",
    "    \n",
    "# class Stupid:\n",
    "#     def __init__(self):\n",
    "#         self.preprocessor = Preprocessor()\n",
    "        \n",
    "#     def chat(self,str):\n",
    "#         return ['I', \"don't\", 'know', '!']\n",
    "    \n",
    "# model = Stupid()\n",
    "# tester = AutomaticMetricTester(model, statements, references)\n",
    "# tester.compileScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tester.printScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Test and Validation data\n",
    "# test_ratio = 0.20\n",
    "\n",
    "# statements = pickle.load(open('../data/genericQuestions.pkl', 'rb'))\n",
    "# references = pickle.load(open('../data/genericAnswers.pkl', 'rb'))\n",
    "\n",
    "# assert len(statements) == len(references)\n",
    "\n",
    "# pairs = list(zip(statements,references))\n",
    "# np.random.shuffle(pairs)\n",
    "\n",
    "# statements = [x for (x,y) in pairs]\n",
    "# references = [y for (x,y) in pairs]\n",
    "\n",
    "# test_index = int(len(pairs)*test_ratio)\n",
    "# pickle.dump(statements[0:test_index], open( \"../data/genericQuestionsTest.pkl\", \"wb\" ) )\n",
    "# pickle.dump(statements[test_index:len(pairs)], open( \"../data/genericQuestionsTrain.pkl\", \"wb\" ))\n",
    "\n",
    "# pickle.dump(references[0:test_index], open( \"../data/genericAnswersTest.pkl\", \"wb\" ) )\n",
    "# pickle.dump(references[test_index:len(pairs)], open( \"../data/genericAnswersTrain.pkl\", \"wb\" ) )\n",
    "\n",
    "\n",
    "# statements = pickle.load(open('../data/joeyInput.pkl', 'rb'))\n",
    "# references = pickle.load(open('../data/joeyOutput.pkl', 'rb'))\n",
    "\n",
    "# assert len(statements) == len(references)\n",
    "\n",
    "# pairs = list(zip(statements,references))\n",
    "# np.random.shuffle(pairs)\n",
    "# statements = [x for (x,y) in pairs]\n",
    "# references = [y for (x,y) in pairs]\n",
    "\n",
    "# test_index = int(len(pairs)*test_ratio)\n",
    "# pickle.dump(statements[0:test_index], open( \"../data/joeyInputTest.pkl\", \"wb\" ) )\n",
    "# pickle.dump(statements[test_index:len(pairs)], open( \"../data/joeyInputTrain.pkl\", \"wb\" ))\n",
    "\n",
    "# pickle.dump(references[0:test_index], open( \"../data/joeyOutputTest.pkl\", \"wb\" ) )\n",
    "# pickle.dump(references[test_index:len(pairs)], open( \"../data/joeyOutputTrain.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
